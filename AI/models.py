import os
import pandas as pd
import numpy as np
import joblib
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModel

# ====================
# 1ï¸âƒ£ í™˜ê²½ ë° ëª¨ë¸ ì„¤ì •
# ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
print("ğŸ“‚ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:", BASE_DIR)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
model_path = os.path.join(BASE_DIR, 'ì²­ì›_ì˜ˆì¸¡ëª¨ë¸.pkl')
scaler_path = os.path.join(BASE_DIR, 'scaler.pkl')
csv_path = os.path.join(BASE_DIR, 'ì§„í–‰ì¤‘êµ­ë¯¼ë™ì˜ì²­ì›í˜„í™©.csv')
crawl_path = os.path.join(BASE_DIR, 'ì²­ì›_ì²˜ë¦¬_í˜„í™©_í¬ë¡¤ë§ì™„ë£Œ.csv')
output_path = os.path.join(BASE_DIR, 'ì˜ˆì¸¡ê²°ê³¼.csv')

# ======================
# 2ï¸âƒ£ ëª¨ë¸ ë° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ======================
try:
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    print("âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ!")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    exit()

try:
    df = pd.read_csv(csv_path, encoding="cp949")
    print(f"âœ… ì§„í–‰ ì¤‘ ì²­ì› {len(df)}ê°œ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ ì²­ì› ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    df = pd.DataFrame(columns=["ì²­ì›ì œëª©", "ì²­ì›ë‚´ìš©"])

try:
    df_crawl = pd.read_csv(crawl_path, encoding="utf-8")
    df = df.merge(df_crawl[["ì²­ì›ì œëª©", "ì²­ì›ë‚´ìš©"]], on="ì²­ì›ì œëª©", how="left")
    print("âœ… í¬ë¡¤ë§ ë°ì´í„° ë³‘í•© ì™„ë£Œ")
except Exception as e:
    print(f"âŒ í¬ë¡¤ë§ ë°ì´í„° ë³‘í•© ì‹¤íŒ¨: {e}")

# ====================
# 3ï¸âƒ£ KoBERT ì„ë² ë”© ìƒì„±
# ====================
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
bert_model = AutoModel.from_pretrained("monologg/kobert", trust_remote_code=True)

def get_bert_embedding(texts):
    embeddings = []
    for text in tqdm(texts, desc="ì„ë² ë”© ì¤‘"):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
        with torch.no_grad():
            outputs = bert_model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
        embeddings.append(cls_embedding)
    return np.array(embeddings)

# ====================
# 4ï¸âƒ£ ì˜ˆì¸¡
# ====================
try:
    df["ì œì¶œì£¼ì²´"] = df["ì²­ì›ì œëª©"].apply(lambda x: 1 if "ë²•ì•ˆ" in str(x) else 0)
    texts = (df["ì²­ì›ì œëª©"].fillna('') + " " + df["ì²­ì›ë‚´ìš©"].fillna('')).tolist()
    embeddings = get_bert_embedding(texts)
    features = np.hstack((embeddings, df[["ì œì¶œì£¼ì²´"]].values))

    scaled_preds = model.predict(features)
    pred_scores = scaler.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()
    df["ìŠ¹ì¸ í™•ë¥ "] = np.round(pred_scores, 2)

    df[["ì²­ì›ì œëª©", "ìŠ¹ì¸ í™•ë¥ "]].to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}")
except Exception as e:
    print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")


