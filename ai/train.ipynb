{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "file_path = \"ì²­ì›_ì²˜ë¦¬_í˜„í™©_í¬ë¡¤ë§ì™„ë£Œ.csv\"\n",
    "train_data = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "# ì²˜ë¦¬ê²°ê³¼ ì ìˆ˜í™” í•¨ìˆ˜\n",
    "def classify_result(result):\n",
    "    mapping = {\n",
    "        \"ì² íšŒ\": 0,\n",
    "        \"ì„ê¸°ë§Œë£Œíê¸°\": 20,\n",
    "        \"ëŒ€ì•ˆë°˜ì˜íê¸°\": 40,\n",
    "        \"ë³¸íšŒì˜ë¶ˆë¶€ì˜\": 60,\n",
    "        \"ë³¸íšŒì˜ì— ë¶€ì˜í•˜ì§€ ì•„ë‹ˆí•˜ê¸°ë¡œ ì˜ê²°\": 80,\n",
    "        \"ì›ì•ˆê°€ê²°\": 100,\n",
    "    }\n",
    "    for key, value in mapping.items():\n",
    "        if key in str(result):\n",
    "            return value\n",
    "    return 50  # ê¸°íƒ€ ì²˜ë¦¬ ê²°ê³¼\n",
    "\n",
    "# ì ìˆ˜í™” ë° ì œì¶œì£¼ì²´ í”Œë˜ê·¸ ìƒì„±\n",
    "train_data[\"ì²˜ë¦¬ê²°ê³¼_ì ìˆ˜\"] = train_data[\"ì˜ê²°ê²°ê³¼\"].apply(classify_result)\n",
    "train_data[\"ì œì¶œì£¼ì²´\"] = train_data[\"ì²­ì›ì œëª©\"].apply(lambda x: 1 if \"ë²•ì•ˆ\" in str(x) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT ì„ë² ë”© ìƒì„±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3730/3730 [01:56<00:00, 32.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# KoBERT ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model_bert = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# BERT ì„ë² ë”© í•¨ìˆ˜\n",
    "def get_bert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"BERT ì„ë² ë”© ìƒì„±\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "train_texts = (train_data[\"ì²­ì›ì œëª©\"] + \" \" + train_data[\"ì²­ì›ë‚´ìš©\"].fillna(\"\"))\n",
    "train_embeddings = get_bert_embedding(train_texts.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… X_train ë° y_train ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# ì…ë ¥(X), ì¶œë ¥(y)\n",
    "X_train = np.hstack((train_embeddings, train_data[[\"ì œì¶œì£¼ì²´\"]].values))\n",
    "y_train = train_data[\"ì²˜ë¦¬ê²°ê³¼_ì ìˆ˜\"].values.reshape(-1, 1)\n",
    "\n",
    "# ì €ì¥ (ì„ íƒ)\n",
    "with open(\"X_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open(\"y_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_train, f)\n",
    "print(\"âœ… X_train ë° y_train ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# ì •ê·œí™”\n",
    "scaler = MinMaxScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train_scaled.ravel())\n",
    "print(\"âœ… XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸(JSON) ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥ (.json)\n",
    "xgb_model.save_model(\"ì²­ì›_ì˜ˆì¸¡ëª¨ë¸.json\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "with open(\"scaler.pkl\", \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "print(\"âœ… ëª¨ë¸(JSON) ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… X_train ë° y_train ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ!\n",
      "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\n",
      "âœ… MAE (Mean Absolute Error): 3.2202\n",
      "âœ… MSE (Mean Squared Error): 18.9943\n",
      "âœ… RMSE (Root Mean Squared Error): 4.3582\n",
      "âœ… RÂ² Score: 0.9444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"X_train.pkl\", \"rb\") as file:\n",
    "    X_train = pickle.load(file)\n",
    "\n",
    "with open(\"y_train.pkl\", \"rb\") as file:\n",
    "    y_train = pickle.load(file)\n",
    "\n",
    "print(\"âœ… X_train ë° y_train ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ!\")\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ ìƒì„± (í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´)\n",
    "y_train_pred_scaled = model.predict(X_train)\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "mae = mean_absolute_error(y_train, y_train_pred)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\")\n",
    "print(f\"âœ… MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"âœ… MSE (Mean Squared Error): {mse:.4f}\")\n",
    "print(f\"âœ… RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"âœ… RÂ² Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
