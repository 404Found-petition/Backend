{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT ÏûÑÎ≤†Îî© ÏÉùÏÑ±: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3730/3730 [01:48<00:00, 34.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ X_train Î∞è y_train Ï†ÄÏû• ÏôÑÎ£å!\n",
      "‚úÖ Î™®Îç∏ Î∞è Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû• ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "file_path = \"Ï≤≠Ïõê_Ï≤òÎ¶¨_ÌòÑÌô©_ÌÅ¨Î°§ÎßÅÏôÑÎ£å.csv\"\n",
    "train_data = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Ï≤òÎ¶¨Í≤∞Í≥º Ï†êÏàòÌôî Ìï®Ïàò\n",
    "def classify_result(result):\n",
    "    mapping = {\n",
    "        \"Ï≤†Ìöå\": 0,\n",
    "        \"ÏûÑÍ∏∞ÎßåÎ£åÌèêÍ∏∞\": 20,\n",
    "        \"ÎåÄÏïàÎ∞òÏòÅÌèêÍ∏∞\": 40,\n",
    "        \"Î≥∏ÌöåÏùòÎ∂àÎ∂ÄÏùò\": 60,\n",
    "        \"Î≥∏ÌöåÏùòÏóê Î∂ÄÏùòÌïòÏßÄ ÏïÑÎãàÌïòÍ∏∞Î°ú ÏùòÍ≤∞\": 80,\n",
    "        \"ÏõêÏïàÍ∞ÄÍ≤∞\": 100,\n",
    "    }\n",
    "    for key, value in mapping.items():\n",
    "        if key in str(result):\n",
    "            return value\n",
    "    return 50  # Í∏∞ÌÉÄ Ï≤òÎ¶¨ Í≤∞Í≥º\n",
    "\n",
    "# Ï†êÏàòÌôî Î∞è Ï†úÏ∂úÏ£ºÏ≤¥ ÌîåÎûòÍ∑∏ ÏÉùÏÑ±\n",
    "train_data[\"Ï≤òÎ¶¨Í≤∞Í≥º_Ï†êÏàò\"] = train_data[\"ÏùòÍ≤∞Í≤∞Í≥º\"].apply(classify_result)\n",
    "train_data[\"Ï†úÏ∂úÏ£ºÏ≤¥\"] = train_data[\"Ï≤≠ÏõêÎ™Ö\"].apply(lambda x: 1 if \"Î≤ïÏïà\" in str(x) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# KoBERT Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model_bert = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# BERT ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "def get_bert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"BERT ÏûÑÎ≤†Îî© ÏÉùÏÑ±\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "train_texts = (train_data[\"Ï≤≠ÏõêÎ™Ö\"] + \" \" + train_data[\"Ï≤≠ÏõêÎÇ¥Ïö©\"].fillna(\"\"))\n",
    "train_embeddings = get_bert_embedding(train_texts.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# ÏûÖÎ†•(X), Ï∂úÎ†•(y)\n",
    "X_train = np.hstack((train_embeddings, train_data[[\"Ï†úÏ∂úÏ£ºÏ≤¥\"]].values))\n",
    "y_train = train_data[\"Ï≤òÎ¶¨Í≤∞Í≥º_Ï†êÏàò\"].values.reshape(-1, 1)\n",
    "\n",
    "# Ï†ÄÏû• (ÏÑ†ÌÉù)\n",
    "with open(\"X_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open(\"y_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_train, f)\n",
    "print(\"‚úÖ X_train Î∞è y_train Ï†ÄÏû• ÏôÑÎ£å!\")\n",
    "\n",
    "# Ï†ïÍ∑úÌôî\n",
    "scaler = MinMaxScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÏùò Î∞è ÌïôÏäµ\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train_scaled.ravel())\n",
    "print(\"‚úÖ XGBoost Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ Ï†ÄÏû• (.json)\n",
    "xgb_model.save_model(\"Ï≤≠Ïõê_ÏòàÏ∏°Î™®Îç∏.json\")\n",
    "\n",
    "# Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•\n",
    "with open(\"scaler.pkl\", \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "print(\"‚úÖ Î™®Îç∏(JSON) Î∞è Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû• ÏôÑÎ£å!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ X_train Î∞è y_train Î∂àÎü¨Ïò§Í∏∞ ÏÑ±Í≥µ!\n",
      "üìä Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥º:\n",
      "‚úÖ MAE (Mean Absolute Error): 3.2202\n",
      "‚úÖ MSE (Mean Squared Error): 18.9943\n",
      "‚úÖ RMSE (Root Mean Squared Error): 4.3582\n",
      "‚úÖ R¬≤ Score: 0.9444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Ï†ÄÏû•Îêú Î™®Îç∏ Î∞è Ïä§ÏºÄÏùºÎü¨ Î∂àÎü¨Ïò§Í∏∞\n",
    "with open(\"X_train.pkl\", \"rb\") as file:\n",
    "    X_train = pickle.load(file)\n",
    "\n",
    "with open(\"y_train.pkl\", \"rb\") as file:\n",
    "    y_train = pickle.load(file)\n",
    "\n",
    "print(\"‚úÖ X_train Î∞è y_train Î∂àÎü¨Ïò§Í∏∞ ÏÑ±Í≥µ!\")\n",
    "\n",
    "# ÏòàÏ∏°Í∞í ÏÉùÏÑ± (ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥)\n",
    "y_train_pred_scaled = model.predict(X_train)\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ÌèâÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "mae = mean_absolute_error(y_train, y_train_pred)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(\"üìä Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥º:\")\n",
    "print(f\"‚úÖ MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"‚úÖ MSE (Mean Squared Error): {mse:.4f}\")\n",
    "print(f\"‚úÖ RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
